# -*- coding: utf-8 -*-
"""03-Customer-Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YUGSOF_6U9vwzOQi1dFOrJaN8GfHjIiE
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("vjchoudhary7/customer-segmentation-tutorial-in-python")

print("Path to dataset files:", path)

# Customer Segmentation using K-Means Clustering
# Identify distinct customer groups based on purchasing behavior

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

print("="*70)
print("CUSTOMER SEGMENTATION ANALYSIS")
print("="*70)

# ============================================
# STEP 1: LOAD THE DATA
# ============================================
# Dataset: Use Mall Customers dataset or Online Retail dataset from Kaggle
# Download links:
# - Mall Customers: https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python
# - Online Retail: https://www.kaggle.com/datasets/vijayuv/onlineretail

print("\n[1/7] Loading customer data...")

# Load customer data
# Expected columns: CustomerID, Gender, Age, Annual Income, Spending Score
df = pd.read_csv('/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')

print(f"Dataset shape: {df.shape}")
print("\nFirst few rows:")
print(df.head())
print("\nDataset Info:")
print(df.info())

# ============================================
# STEP 2: DATA EXPLORATION
# ============================================
print("\n" + "="*70)
print("[2/7] Exploring the data...")
print("="*70)

# Basic statistics
print("\nStatistical Summary:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Data distribution
print("\nData Distribution:")
if 'Gender' in df.columns:
    print(f"\nGender Distribution:")
    print(df['Gender'].value_counts())

# Check column names and adjust based on your dataset
# Common column names: Age, Annual Income (k$), Spending Score (1-100)
print("\nColumn Names:")
print(df.columns.tolist())

# ============================================
# STEP 3: DATA PREPROCESSING
# ============================================
print("\n" + "="*70)
print("[3/7] Preprocessing data...")
print("="*70)

# Remove any duplicates
df = df.drop_duplicates()
print(f"âœ“ Duplicates removed. Shape: {df.shape}")

# Handle missing values if any
df = df.dropna()

# Create a copy for clustering (we'll use numerical features only)
df_clustering = df.copy()

# Encode categorical variables if present
if 'Gender' in df_clustering.columns:
    df_clustering['Gender_Encoded'] = df_clustering['Gender'].map({'Male': 0, 'Female': 1})

# Select features for clustering
# Adjust these column names based on your dataset
feature_columns = []

# Try to identify the right columns automatically
for col in df_clustering.columns:
    if df_clustering[col].dtype in ['int64', 'float64'] and col not in ['CustomerID', 'Customer ID']:
        feature_columns.append(col)

print(f"\nFeatures selected for clustering: {feature_columns}")

# If specific columns exist, use them
if 'Annual Income (k$)' in df_clustering.columns and 'Spending Score (1-100)' in df_clustering.columns:
    # For Mall Customers dataset
    features = df_clustering[['Annual Income (k$)', 'Spending Score (1-100)']]
    feature_names = ['Annual Income', 'Spending Score']
elif 'Age' in df_clustering.columns and len(feature_columns) >= 2:
    # Use available numerical features
    features = df_clustering[feature_columns[:3]]  # Use first 3 numerical features
    feature_names = feature_columns[:3]
else:
    # Use all numerical features
    features = df_clustering[feature_columns]
    feature_names = feature_columns

print(f"\nUsing features: {list(features.columns)}")
print(f"\nFeature statistics:")
print(features.describe())

# ============================================
# STEP 4: FEATURE SCALING
# ============================================
print("\n" + "="*70)
print("[4/7] Scaling features...")
print("="*70)

# Standardize features (important for K-means)
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

print("âœ“ Features scaled using StandardScaler")
print(f"Scaled data shape: {features_scaled.shape}")

# ============================================
# STEP 5: FIND OPTIMAL NUMBER OF CLUSTERS
# ============================================
print("\n" + "="*70)
print("[5/7] Finding optimal number of clusters...")
print("="*70)

# Elbow Method
inertias = []
silhouette_scores = []
K_range = range(2, 11)

print("Testing different numbers of clusters...")
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(features_scaled)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(features_scaled, kmeans.labels_))
    print(f"  K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.3f}")

# Plot Elbow Curve
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# Elbow Method Plot
ax1.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
ax1.set_xlabel('Number of Clusters (K)', fontsize=12)
ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)
ax1.set_title('Elbow Method - Finding Optimal K', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3)

# Silhouette Score Plot
ax2.plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)
ax2.set_xlabel('Number of Clusters (K)', fontsize=12)
ax2.set_ylabel('Silhouette Score', fontsize=12)
ax2.set_title('Silhouette Score vs Number of Clusters', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Suggest optimal K
optimal_k_silhouette = K_range[np.argmax(silhouette_scores)]
print(f"\nâœ“ Suggested optimal K based on Silhouette Score: {optimal_k_silhouette}")
print(f"  You can also choose K based on the 'elbow' in the first plot")

# ============================================
# STEP 6: APPLY K-MEANS CLUSTERING
# ============================================
print("\n" + "="*70)
print("[6/7] Applying K-Means clustering...")
print("="*70)

# Use optimal K (you can change this based on the plots)
optimal_k = 5  # Change this based on your elbow plot observation
print(f"Using K = {optimal_k} clusters")

# Fit K-means
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(features_scaled)

print(f"âœ“ Clustering complete!")
print(f"\nCluster Distribution:")
print(df['Cluster'].value_counts().sort_index())

# Get cluster centers (in original scale)
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
centers_df = pd.DataFrame(cluster_centers, columns=features.columns)
centers_df['Cluster'] = range(optimal_k)

print(f"\nCluster Centers (Average values):")
print(centers_df)

# ============================================
# STEP 7: VISUALIZE CLUSTERS
# ============================================
print("\n" + "="*70)
print("[7/7] Creating visualizations...")
print("="*70)

# If we have 2 features, plot directly
if features.shape[1] == 2:
    plt.figure(figsize=(12, 8))

    # Plot clusters
    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']
    for i in range(optimal_k):
        cluster_data = df[df['Cluster'] == i]
        plt.scatter(
            cluster_data.iloc[:, -3],  # Assuming income is second-to-last before cluster
            cluster_data.iloc[:, -2],  # Assuming spending is last before cluster
            c=colors[i],
            label=f'Cluster {i}',
            alpha=0.6,
            s=100,
            edgecolors='black',
            linewidth=0.5
        )

    # Plot cluster centers
    plt.scatter(
        cluster_centers[:, 0],
        cluster_centers[:, 1],
        c='black',
        s=300,
        alpha=0.8,
        marker='X',
        edgecolors='white',
        linewidth=2,
        label='Centroids'
    )

    plt.xlabel(features.columns[0], fontsize=12)
    plt.ylabel(features.columns[1], fontsize=12)
    plt.title('Customer Segments Visualization', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

else:
    # Use PCA for dimensionality reduction if more than 2 features
    print("Using PCA for visualization (more than 2 features)...")
    pca = PCA(n_components=2)
    features_pca = pca.fit_transform(features_scaled)

    print(f"PCA Explained Variance: {pca.explained_variance_ratio_}")
    print(f"Total Variance Explained: {sum(pca.explained_variance_ratio_):.2%}")

    plt.figure(figsize=(12, 8))

    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']
    for i in range(optimal_k):
        cluster_mask = df['Cluster'] == i
        plt.scatter(
            features_pca[cluster_mask, 0],
            features_pca[cluster_mask, 1],
            c=colors[i],
            label=f'Cluster {i}',
            alpha=0.6,
            s=100,
            edgecolors='black',
            linewidth=0.5
        )

    plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)
    plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)
    plt.title('Customer Segments (PCA Visualization)', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

# ============================================
# CLUSTER PROFILING
# ============================================
print("\n" + "="*70)
print("CLUSTER PROFILING")
print("="*70)

# Analyze each cluster
for cluster_id in range(optimal_k):
    cluster_data = df[df['Cluster'] == cluster_id]
    print(f"\n{'='*70}")
    print(f"CLUSTER {cluster_id} - ({len(cluster_data)} customers, {len(cluster_data)/len(df)*100:.1f}%)")
    print('='*70)

    # Numerical features statistics
    print("\nKey Characteristics:")
    for col in features.columns:
        mean_val = cluster_data[col].mean()
        print(f"  â€¢ Average {col}: {mean_val:.2f}")

    # Categorical features if present
    if 'Gender' in df.columns:
        gender_dist = cluster_data['Gender'].value_counts()
        print(f"\n  Gender Distribution:")
        for gender, count in gender_dist.items():
            print(f"    - {gender}: {count} ({count/len(cluster_data)*100:.1f}%)")

# ============================================
# CREATE SEGMENT DESCRIPTIONS
# ============================================
print("\n" + "="*70)
print("SEGMENT DESCRIPTIONS & INSIGHTS")
print("="*70)

# Automatic segment naming based on characteristics
def describe_segment(cluster_id, cluster_data, features):
    """Generate a descriptive name for each segment"""
    descriptions = []

    for col in features.columns:
        mean_val = cluster_data[col].mean()
        overall_mean = df[col].mean()

        if mean_val > overall_mean * 1.2:
            descriptions.append(f"High {col}")
        elif mean_val < overall_mean * 0.8:
            descriptions.append(f"Low {col}")

    return " & ".join(descriptions) if descriptions else "Average Profile"

print("\nSuggested Segment Names:")
for cluster_id in range(optimal_k):
    cluster_data = df[df['Cluster'] == cluster_id]
    description = describe_segment(cluster_id, cluster_data, features)
    print(f"  Cluster {cluster_id}: {description}")

# ============================================
# DISTRIBUTION PLOTS
# ============================================
print("\n" + "="*70)
print("Creating distribution plots...")
print("="*70)

# Create distribution plots for each feature by cluster
num_features = len(features.columns)
fig, axes = plt.subplots(1, min(num_features, 3), figsize=(15, 5))

if num_features == 1:
    axes = [axes]
elif num_features == 2:
    axes = axes.flatten()
else:
    axes = axes.flatten()[:num_features]

colors = ['red', 'blue', 'green', 'orange', 'purple']

for idx, col in enumerate(features.columns[:3]):  # Plot first 3 features
    for cluster_id in range(optimal_k):
        cluster_data = df[df['Cluster'] == cluster_id][col]
        axes[idx].hist(cluster_data, alpha=0.5, label=f'Cluster {cluster_id}',
                      color=colors[cluster_id], bins=15, edgecolor='black')

    axes[idx].set_xlabel(col, fontsize=10)
    axes[idx].set_ylabel('Frequency', fontsize=10)
    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')
    axes[idx].legend(fontsize=8)
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ============================================
# SAVE RESULTS
# ============================================
print("\n" + "="*70)
print("Saving results...")
print("="*70)

# Save clustered data
output_file = 'customer_segments.csv'
df.to_csv(output_file, index=False)
print(f"âœ“ Clustered data saved to: {output_file}")

# Save cluster summary
summary_file = 'cluster_summary.csv'
centers_df.to_csv(summary_file, index=False)
print(f"âœ“ Cluster summary saved to: {summary_file}")

# ============================================
# FINAL SUMMARY
# ============================================
print("\n" + "="*70)
print("ANALYSIS COMPLETE! ðŸŽ‰")
print("="*70)

print(f"\nðŸ“Š Summary:")
print(f"   â€¢ Total customers analyzed: {len(df):,}")
print(f"   â€¢ Number of segments identified: {optimal_k}")
print(f"   â€¢ Features used: {', '.join(features.columns)}")
print(f"   â€¢ Silhouette Score: {silhouette_score(features_scaled, df['Cluster']):.3f}")

print(f"\nðŸ’¡ Next Steps:")
print(f"   1. Review cluster profiles above")
print(f"   2. Assign marketing strategies to each segment")
print(f"   3. Use segments for targeted campaigns")
print(f"   4. Monitor segment changes over time")

print(f"\nðŸ“ Output Files:")
print(f"   â€¢ {output_file} - Full dataset with cluster assignments")
print(f"   â€¢ {summary_file} - Cluster centers and characteristics")